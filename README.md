# EmoCues/EmoCause

# EmoVideoLLaMA

<!--EmoVideoLLaMA() æ˜¯åœ¨EmoCuesä¸ŠæŒ‡ä»¤å¾®è°ƒï¼Œç„¶åŽåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šæ ‡ç­¾å¾®è°ƒçš„å¤šæ¨¡æ€æƒ…æ„Ÿå¤§æ¨¡åž‹ã€‚-->
EmoVideoLLaMA () is a multimodal emotional large-scale model that instructs fine tuning on EmoCues and then labels fine tuning on a specific data set.
EmoVideoLLaMA is already test on YE-8 and YF-6 dataset.

# ToDo List
- [ ] Supprot more MLLMs(audio-visual).
- [ ] New Benchmark.
- [ ] Support more datasets (MediaEval2016)

## ðŸ‘ Acknowledgement
The codebase of EmoVideoLLaMA 2 is adapted from [**LLaVA 1.5**](https:github.com/haotian-liu/LLaVA) and [**FastChat**](https://github.com/lm-sys/FastChat). We are also grateful for the following projects our EmoVideoLLaMA 2 arise from:
* [**VideoLLaMA2**](https://github.com/DAMO-NLP-SG/VideoLLaMA2)
* [**LLaMA 2**](https://github.com/meta-llama/llama), [**Mistral-7B**](https://mistral.ai/news/announcing-mistral-7b/), [**OpenAI CLIP**](https://openai.com/index/clip/), [**Honeybee**](https://github.com/kakaobrain/honeybee).
* [**Video-ChatGPT**](https://github.com/mbzuai-oryx/Video-ChatGPT), [**Video-LLaVA**](https://github.com/PKU-YuanGroup/Video-LLaVA). 
* [**WebVid**](https://github.com/m-bain/webvid), [**Panda-70M**](https://github.com/snap-research/Panda-70M), [**LanguageBind**](https://github.com/PKU-YuanGroup/LanguageBind), [**InternVid**](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid).
* [**VideoChat2**](https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2), [**Valley**](https://github.com/RupertLuo/Valley), [**VTimeLLM**](https://github.com/huangb23/VTimeLLM), [**ShareGPT4V**](https://sharegpt4v.github.io/).


## ðŸ”’ License

This project is released under the Apache 2.0 license as found in the LICENSE file.
The service is a research preview intended for **non-commercial use ONLY**, subject to the model Licenses of LLaMA and Mistral, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Please get in touch with us if you find any potential violations. -->
